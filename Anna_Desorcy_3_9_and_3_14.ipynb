{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anna-Desorcy/FakeNewsDetection/blob/main/Anna_Desorcy_3_9_and_3_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees Part 2\n",
        "\n",
        "https://tinyurl.com/4mfb63zw"
      ],
      "metadata": {
        "id": "xevf6CJgnrl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3XSSlFqnjwR",
        "outputId": "4c771939-f389-423e-8f37-e049fc65c1ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age  Experience  Rank Nationality   Go\n",
            "0    36          10     9          UK   NO\n",
            "1    42          12     4         USA   NO\n",
            "2    23           4     6           N   NO\n",
            "3    52           4     4         USA   NO\n",
            "4    43          21     8         USA  YES\n",
            "5    44          14     5          UK   NO\n",
            "6    66           3     7           N  YES\n",
            "7    35          14     9          UK  YES\n",
            "8    52          13     7           N  YES\n",
            "9    35           5     9           N  YES\n",
            "10   24           3     5         USA   NO\n",
            "11   18           3     7          UK  YES\n",
            "12   45           9     9          UK  YES\n"
          ]
        }
      ],
      "source": [
        "#Read and print the dataset\n",
        "import pandas\n",
        "\n",
        "df = pandas.read_csv(\"data.csv\")\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a decision tree, all data has to be numerical.\n",
        "\n",
        "We have to convert the non numerical columns 'Nationality' and 'Go' into numerical values.\n",
        "\n",
        "Pandas has a map() method that takes a dictionary with information on how to convert the values.\n",
        "\n",
        "{'UK': 0, 'USA': 1, 'N': 2}\n",
        "\n",
        "Means convert the values 'UK' to 0, 'USA' to 1, and 'N' to 2."
      ],
      "metadata": {
        "id": "UCDd5xxvn1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change string values into numerical values:\n",
        "d = {'UK': 0, 'USA': 1, 'N': 2}\n",
        "df['Nationality'] = df['Nationality'].map(d)\n",
        "d = {'YES': 1, 'NO': 0}\n",
        "df['Go'] = df['Go'].map(d)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2M2PZ83n8Sn",
        "outputId": "6610c012-7183-415d-a54f-3ee3c1260844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age  Experience  Rank  Nationality  Go\n",
            "0    36          10     9            0   0\n",
            "1    42          12     4            1   0\n",
            "2    23           4     6            2   0\n",
            "3    52           4     4            1   0\n",
            "4    43          21     8            1   1\n",
            "5    44          14     5            0   0\n",
            "6    66           3     7            2   1\n",
            "7    35          14     9            0   1\n",
            "8    52          13     7            2   1\n",
            "9    35           5     9            2   1\n",
            "10   24           3     5            1   0\n",
            "11   18           3     7            0   1\n",
            "12   45           9     9            0   1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to separate the feature columns from the target column.\n",
        "\n",
        "The feature columns are the columns that we try to predict from, and the target column is the column with the values we try to predict."
      ],
      "metadata": {
        "id": "9W3GtKMfoCD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X is the feature columns, y is the target column:\n",
        "features = ['Age', 'Experience', 'Rank', 'Nationality']\n",
        "\n",
        "X = df[features]\n",
        "y = df['Go']\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZEfWHqGoDJz",
        "outputId": "2a6936cc-e35b-49cd-8cc7-3cdfefe3b9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Age  Experience  Rank  Nationality\n",
            "0    36          10     9            0\n",
            "1    42          12     4            1\n",
            "2    23           4     6            2\n",
            "3    52           4     4            1\n",
            "4    43          21     8            1\n",
            "5    44          14     5            0\n",
            "6    66           3     7            2\n",
            "7    35          14     9            0\n",
            "8    52          13     7            2\n",
            "9    35           5     9            2\n",
            "10   24           3     5            1\n",
            "11   18           3     7            0\n",
            "12   45           9     9            0\n",
            "0     0\n",
            "1     0\n",
            "2     0\n",
            "3     0\n",
            "4     1\n",
            "5     0\n",
            "6     1\n",
            "7     1\n",
            "8     1\n",
            "9     1\n",
            "10    0\n",
            "11    1\n",
            "12    1\n",
            "Name: Go, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create and display a Decision Tree:\n",
        "import pandas\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pandas.read_csv(\"data.csv\")\n",
        "\n",
        "d = {'UK': 0, 'USA': 1, 'N': 2}\n",
        "df['Nationality'] = df['Nationality'].map(d)\n",
        "d = {'YES': 1, 'NO': 0}\n",
        "df['Go'] = df['Go'].map(d)\n",
        "\n",
        "features = ['Age', 'Experience', 'Rank', 'Nationality']\n",
        "\n",
        "X = df[features]\n",
        "y = df['Go']\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X, y)\n",
        "\n",
        "#tree.plot_tree(dtree, feature_names=features)"
      ],
      "metadata": {
        "id": "EnOqmjR-oJkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Explained\n",
        "The decision tree uses your earlier decisions to calculate the odds for you to wanting to go see a comedian or not.\n",
        "\n",
        "Let us read the different aspects of the decision tree:\n",
        "\n",
        "Rank <= 6.5 means that every comedian with a rank of 6.5 or lower will follow the True arrow (to the left), and the rest will follow the False arrow (to the right).\n",
        "\n",
        "gini = 0.497 refers to the quality of the split, and is always a number between 0.0 and 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.\n",
        "\n",
        "samples = 13 means that there are 13 comedians left at this point in the decision, which is all of them since this is the first step.\n",
        "\n",
        "value = [6, 7] means that of these 13 comedians, 6 will get a \"NO\", and 7 will get a \"GO\".\n",
        "\n",
        "The next step contains two boxes, one box for the comedians with a 'Rank' of 6.5 or lower, and one box with the rest.\n",
        "\n",
        "### True - 5 Comedians End Here:\n",
        "gini = 0.0 means all of the samples got the same result.\n",
        "\n",
        "samples = 5 means that there are 5 comedians left in this branch (5 comedian with a Rank of 6.5 or lower).\n",
        "\n",
        "value = [5, 0] means that 5 will get a \"NO\" and 0 will get a \"GO\".\n",
        "\n",
        "###False - 8 Comedians Continue:\n",
        "\n",
        "Nationality <= 0.5 means that the comedians with a nationality value of less than 0.5 will follow the arrow to the left (which means everyone from the UK, ), and the rest will follow the arrow to the right.\n",
        "\n",
        "gini = 0.219 means that about 22% of the samples would go in one direction.\n",
        "\n",
        "samples = 8 means that there are 8 comedians left in this branch (8 comedian with a Rank higher than 6.5).\n",
        "\n",
        "value = [1, 7] means that of these 8 comedians, 1 will get a \"NO\" and 7 will get a \"GO\".\n",
        "\n",
        "\n",
        "... AND SO ON"
      ],
      "metadata": {
        "id": "4hA4cb3poP1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Values\n",
        "We can use the Decision Tree to predict new values.\n",
        "\n",
        "Example: Should I go see a show starring a 40 years old American comedian, with 10 years of experience, and a comedy ranking of 7?"
      ],
      "metadata": {
        "id": "7CIsXSo8osoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use predict() method to predict new values:\n",
        "print(dtree.predict([[40, 10, 7, 1]]))\n"
      ],
      "metadata": {
        "id": "pUkk32B2ovWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1897bcc2-1106-4411-e1e8-214bd22f9494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning News Data\n",
        "We need to clean the news data so it is usable by the decision tree algorithm/model."
      ],
      "metadata": {
        "id": "LhZkmk8gpo7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip fake news data\n",
        "!unzip \"News _dataset.zip\""
      ],
      "metadata": {
        "id": "eDHM6ECKpxsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a01d150-f2c0-4102-fcea-992d53c146ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  News _dataset.zip\n",
            "  inflating: Fake.csv                \n",
            "  inflating: True.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the data\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Fake.csv\")\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "QDwP7dJvq4RH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832cf4ae-b668-4451-f9b4-4bf1e4f1d6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
            "\n",
            "                                                text subject  \\\n",
            "0  Donald Trump just couldn t wish all Americans ...    News   \n",
            "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
            "2  On Friday, it was revealed that former Milwauk...    News   \n",
            "3  On Christmas day, Donald Trump announced that ...    News   \n",
            "4  Pope Francis used his annual Christmas Day mes...    News   \n",
            "\n",
            "                date  \n",
            "0  December 31, 2017  \n",
            "1  December 31, 2017  \n",
            "2  December 30, 2017  \n",
            "3  December 29, 2017  \n",
            "4  December 25, 2017  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words\n",
        "### Step 1: Collect Data\n",
        "Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.\n",
        "\n",
        "It was the best of times,\n",
        "it was the worst of times,\n",
        "it was the age of wisdom,\n",
        "it was the age of foolishness,\n",
        "\n",
        "For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n",
        "\n",
        "###Step 2: Design the Vocabulary\n",
        "Now we can make a list of all of the words in our model vocabulary.\n",
        "\n",
        "The unique words here (ignoring case and punctuation) are:\n",
        "\n",
        "\n",
        "*   \"it\"\n",
        "*   \"was\"\n",
        "* \"the\"\n",
        "* \"best\"\n",
        "* \"of\"\n",
        "* \"times\"\n",
        "* \"worst\"\n",
        "* \"age\"\n",
        "* \"wisdom\"\n",
        "* \"foolishness\"\n",
        "\n",
        "That is a vocabulary of 10 words from a corpus containing 24 words.\n",
        "\n",
        "###Step 3: Create Document Vectors\n",
        "The next step is to score the words in each document.\n",
        "\n",
        "The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.\n",
        "\n",
        "Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.\n",
        "\n",
        "The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n",
        "\n",
        "Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.\n",
        "\n",
        "The scoring of the document would look as follows:\n",
        "\n",
        "*   \"it\" = 1\n",
        "*   \"was\" = 1\n",
        "* \"the\" = 1\n",
        "* \"best\" = 1\n",
        "* \"of\" = 1\n",
        "* \"times\" = 1\n",
        "* \"worst\" = 0\n",
        "* \"age\" = 0\n",
        "* \"wisdom\" = 0\n",
        "* \"foolishness\" = 0\n",
        "\n",
        "As a binary vector, this would look as follows:\n",
        "```\n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "```\n",
        "\n",
        "The other three documents would look as follows:\n",
        "\n",
        "```\n",
        "\"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
        "\"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "\"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
        "```"
      ],
      "metadata": {
        "id": "-gXrf2aBuoou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_article(text):\n",
        "  #remove punctuation\n",
        "  text = text.lower()\n",
        "  text = text.replace('.','')\n",
        "  text = text.replace(',','')\n",
        "  text = text.replace('!','')\n",
        "  text = text.replace('\"','')\n",
        "  text = text.replace(\"'\",'')\n",
        "  text = text.replace('?','')\n",
        "  text = text.replace(':','')\n",
        "  text = text.replace('/','')\n",
        "  text = text.replace('@','')\n",
        "  text = text.replace('(','')\n",
        "  text = text.replace(')','')\n",
        "  text = text.replace('[','')\n",
        "  text = text.replace(']','')\n",
        "  text = text.replace('_','')\n",
        "  text = text.replace('*','')\n",
        "  text = text.replace('0','')\n",
        "  text = text.replace('1','')\n",
        "  text = text.replace('2','')\n",
        "  text = text.replace('3','')\n",
        "  text = text.replace('4','')\n",
        "  text = text.replace('5','')\n",
        "  text = text.replace('6','')\n",
        "  text = text.replace('7','')\n",
        "  text = text.replace('8','')\n",
        "  text = text.replace('9','')\n",
        "  text = text.replace('-','')\n",
        "  text = text.replace('#','')\n",
        "  text = text.replace(';','')\n",
        "\n",
        "  #split into words\n",
        "  text = text.strip().split()\n",
        "\n",
        "  #remove links\n",
        "  text = [ x for x in text if \"www\" not in x ]\n",
        "  text = [ x for x in text if \"http\" not in x ]\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "Uhnhjj1zc8vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turn Fake and Real news into bag of words (use the text from the article)\n",
        "\n",
        "#You will need to use both sets of data to create a common vocabulary\n",
        "\n",
        "#I recommend using a dictionary, rather than a list, to keep track of your BOW\n",
        "\n",
        "#Save your data in a new CSV file (news_data.csv) that has the vocabulary as\n",
        "#the header and the counts as the features for each article\n",
        "\n",
        "#We're going to use this data csv on Thursday to train our decision tree model\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "word_dict = {}\n",
        "\n",
        "#Get Vocab Words for Fake\n",
        "cnt = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "  cnt += 1\n",
        "  if cnt > 1000:\n",
        "    break\n",
        "\n",
        "#Get Vocab Words for Real\n",
        "cnt = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "  cnt += 1\n",
        "  if cnt > 1000:\n",
        "    break\n",
        "\n",
        "#Remove words that occur less than min_thresh times and more than max_thresh times\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length Before Min/Max Removal:\", len(vocab))\n",
        "\n",
        "min_thresh = 100\n",
        "max_thresh = 1000\n",
        "for word in vocab:\n",
        "  if word_dict[word] <= min_thresh or word_dict[word] > max_thresh:\n",
        "    word_dict.pop(word)\n",
        "\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length After Min/Max Removal:\", len(vocab))\n"
      ],
      "metadata": {
        "id": "z8avBB6Wudg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee25390-f52d-41c9-b774-8e439132bdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Length Before Min/Max Removal: 36871\n",
            "Vocabulary Length After Min/Max Removal: 885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now write out BOW for each article\n",
        "\n",
        "#create empty article dictionary\n",
        "article_dict = word_dict.copy()\n",
        "article_dict = dict.fromkeys(article_dict, 0) #This is faster than looping through and setting each count to 0\n",
        "\n",
        "#Open output file and write the vocab out as the header line\n",
        "fout = open('news_data.csv', 'w')\n",
        "vocab_str = ','.join(vocab)\n",
        "fout.write(vocab_str + ',target_label\\n') #add target_label header for label column\n",
        "\n",
        "cnt = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "\n",
        "  #Turn count list into a string of comma separated values\n",
        "  article_list = list(article_dict.values())\n",
        "  str_list = ','.join(str(e) for e in article_list)\n",
        "  fout.write(str_list + ',1\\n') #add 1 at the end for label for fake\n",
        "\n",
        "  #reset article dictionary to 0 counts\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  #only keep the first 1000 articles\n",
        "  cnt += 1\n",
        "  if cnt >= 1000:\n",
        "    break\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "cnt = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  article_list = list(article_dict.values())\n",
        "  str_list = ','.join(str(e) for e in article_list)\n",
        "  fout.write(str_list + ',0\\n')\n",
        "\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "  cnt += 1\n",
        "  if cnt >= 1000:\n",
        "    break\n",
        "fout.close()"
      ],
      "metadata": {
        "id": "krwxF5Eth7x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create and display a Decision Tree:\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "df_train = pd.read_csv('news_data.csv')\n",
        "#get feature names\n",
        "#get X and y (use dictionary_name.values so you don't get all those warnings)\n",
        "list_of_features = df_train.keys()[:-1]\n",
        "X = df_train[list_of_features].values\n",
        "y = df_train['target_label']\n",
        "\n",
        "#train DT\n",
        "\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X, y)\n",
        "\n",
        "#print DT\n",
        "tree.plot_tree(dtree, feature_names=list_of_features)\n"
      ],
      "metadata": {
        "id": "uYVuKZazCvQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test our decision tree on more data from Fake.csv and True.csv\n",
        "\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "\n",
        "df_fake = df_fake[1000:10000]\n",
        "df_real = df_real[1000:10000]\n",
        "\n",
        "#test samples 1000-1100 from Fake and True and get accuracies for both\n",
        "#test samples 1000-10000 from Fake and Real and get accuracies for both\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if dtree.predict([article_list]) == 1:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Fake Data Test Accuracy:  {round((correct / total) * 100, 2)}%')\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if dtree.predict([article_list]) == 0:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Real Data Test Accuracy:  {round((correct / total) * 100, 2)}%')"
      ],
      "metadata": {
        "id": "ppaW8I7sSTRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fb6bc5-b0eb-410e-dbb9-1f3506ae1daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake Data Test Accuracy:  93.91%\n",
            "Real Data Test Accuracy:  98.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "df_train = pd.read_csv('news_data.csv')\n",
        "\n",
        "list_of_features = df_train.keys()[:-1]\n",
        "X = df_train[list_of_features].values\n",
        "y = df_train['target_label']\n",
        "\n",
        "#tree.plot_tree(dtree, feature_names=list_of_features)\n",
        "\n",
        "#Train KNN\n",
        "KNN = KNeighborsClassifier(n_neighbors = 3)\n",
        "KNN = KNN.fit(X,y)\n"
      ],
      "metadata": {
        "id": "d--n1h5IP7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "\n",
        "df_fake = df_fake[1000:10000]\n",
        "df_real = df_real[1000:10000]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if KNN.predict([article_list]) == 1:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Fake Data Test Accuracy:  {round((correct / total) * 100, 2)}%')\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if KNN.predict([article_list]) == 0:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Real Data Test Accuracy:  {round((correct / total) * 100, 2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrXBK8eCRpDm",
        "outputId": "114735ca-d766-4ffe-bc18-f037d4d1f0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake Data Test Accuracy:  16.62%\n",
            "Real Data Test Accuracy:  95.63%\n"
          ]
        }
      ]
    }
  ]
}
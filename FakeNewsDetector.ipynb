{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anna-Desorcy/FakeNewsDetection/blob/main/FakeNewsDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake News Detector\n",
        "\n"
      ],
      "metadata": {
        "id": "LhZkmk8gpo7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipping fake news data\n",
        "!unzip \"News _dataset.zip\""
      ],
      "metadata": {
        "id": "eDHM6ECKpxsY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a01d150-f2c0-4102-fcea-992d53c146ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  News _dataset.zip\n",
            "  inflating: Fake.csv                \n",
            "  inflating: True.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading in the data\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"Fake.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "QDwP7dJvq4RH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832cf4ae-b668-4451-f9b4-4bf1e4f1d6cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
            "\n",
            "                                                text subject  \\\n",
            "0  Donald Trump just couldn t wish all Americans ...    News   \n",
            "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
            "2  On Friday, it was revealed that former Milwauk...    News   \n",
            "3  On Christmas day, Donald Trump announced that ...    News   \n",
            "4  Pope Francis used his annual Christmas Day mes...    News   \n",
            "\n",
            "                date  \n",
            "0  December 31, 2017  \n",
            "1  December 31, 2017  \n",
            "2  December 30, 2017  \n",
            "3  December 29, 2017  \n",
            "4  December 25, 2017  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_article(text):\n",
        "  #remove punctuation\n",
        "  text = text.lower()\n",
        "  text = text.replace('.','')\n",
        "  text = text.replace(',','')\n",
        "  text = text.replace('!','')\n",
        "  text = text.replace('\"','')\n",
        "  text = text.replace(\"'\",'')\n",
        "  text = text.replace('?','')\n",
        "  text = text.replace(':','')\n",
        "  text = text.replace('/','')\n",
        "  text = text.replace('@','')\n",
        "  text = text.replace('(','')\n",
        "  text = text.replace(')','')\n",
        "  text = text.replace('[','')\n",
        "  text = text.replace(']','')\n",
        "  text = text.replace('_','')\n",
        "  text = text.replace('*','')\n",
        "  text = text.replace('0','')\n",
        "  text = text.replace('1','')\n",
        "  text = text.replace('2','')\n",
        "  text = text.replace('3','')\n",
        "  text = text.replace('4','')\n",
        "  text = text.replace('5','')\n",
        "  text = text.replace('6','')\n",
        "  text = text.replace('7','')\n",
        "  text = text.replace('8','')\n",
        "  text = text.replace('9','')\n",
        "  text = text.replace('-','')\n",
        "  text = text.replace('#','')\n",
        "  text = text.replace(';','')\n",
        "\n",
        "  #split into words\n",
        "  text = text.strip().split()\n",
        "\n",
        "  #remove links\n",
        "  text = [ x for x in text if \"www\" not in x ]\n",
        "  text = [ x for x in text if \"http\" not in x ]\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "Uhnhjj1zc8vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turning Fake and Real news into bag of words\n",
        "import pandas as pd\n",
        "\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "word_dict = {}\n",
        "\n",
        "#Get Vocab Words for Fake\n",
        "cnt = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "  cnt += 1\n",
        "  if cnt > 1000:\n",
        "    break\n",
        "\n",
        "#Get Vocab Words for Real\n",
        "cnt = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      word_dict[word] += 1\n",
        "    except:\n",
        "      word_dict[word] = 0\n",
        "  cnt += 1\n",
        "  if cnt > 1000:\n",
        "    break\n",
        "\n",
        "#Remove words that occur less than min_thresh times and more than max_thresh times\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length Before Min/Max Removal:\", len(vocab))\n",
        "\n",
        "min_thresh = 100\n",
        "max_thresh = 1000\n",
        "for word in vocab:\n",
        "  if word_dict[word] <= min_thresh or word_dict[word] > max_thresh:\n",
        "    word_dict.pop(word)\n",
        "\n",
        "vocab = list(word_dict)\n",
        "print(\"Vocabulary Length After Min/Max Removal:\", len(vocab))\n"
      ],
      "metadata": {
        "id": "z8avBB6Wudg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee25390-f52d-41c9-b774-8e439132bdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Length Before Min/Max Removal: 36871\n",
            "Vocabulary Length After Min/Max Removal: 885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing out BOW for each article\n",
        "\n",
        "#create empty article dictionary\n",
        "article_dict = word_dict.copy()\n",
        "article_dict = dict.fromkeys(article_dict, 0) #This is faster than looping through and setting each count to 0\n",
        "\n",
        "#Open output file and write the vocab out as the header line\n",
        "fout = open('news_data.csv', 'w')\n",
        "vocab_str = ','.join(vocab)\n",
        "fout.write(vocab_str + ',target_label\\n') #add target_label header for label column\n",
        "\n",
        "cnt = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "\n",
        "  #Turn count list into a string of comma separated values\n",
        "  article_list = list(article_dict.values())\n",
        "  str_list = ','.join(str(e) for e in article_list)\n",
        "  fout.write(str_list + ',1\\n') #add 1 at the end for label for fake\n",
        "\n",
        "  #reset article dictionary to 0 counts\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  #only keep the first 1000 articles for quicker computation\n",
        "  cnt += 1\n",
        "  if cnt >= 1000:\n",
        "    break\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "cnt = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  article_list = list(article_dict.values())\n",
        "  str_list = ','.join(str(e) for e in article_list)\n",
        "  fout.write(str_list + ',0\\n')\n",
        "\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "  cnt += 1\n",
        "  if cnt >= 1000:\n",
        "    break\n",
        "fout.close()"
      ],
      "metadata": {
        "id": "krwxF5Eth7x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create and display a Decision Tree:\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "df_train = pd.read_csv('news_data.csv')\n",
        "#get feature names\n",
        "#get X and y (use dictionary_name.values so you don't get all those warnings)\n",
        "list_of_features = df_train.keys()[:-1]\n",
        "X = df_train[list_of_features].values\n",
        "y = df_train['target_label']\n",
        "\n",
        "#train DT\n",
        "dtree = DecisionTreeClassifier()\n",
        "dtree = dtree.fit(X, y)\n",
        "\n",
        "#print DT\n",
        "tree.plot_tree(dtree, feature_names=list_of_features)\n"
      ],
      "metadata": {
        "id": "uYVuKZazCvQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test our decision tree on more data from Fake.csv and True.csv\n",
        "\n",
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "\n",
        "df_fake = df_fake[1000:10000]\n",
        "df_real = df_real[1000:10000]\n",
        "\n",
        "#test samples 1000-1100 from Fake and True and get accuracies for both\n",
        "#test samples 1000-10000 from Fake and Real and get accuracies for both\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if dtree.predict([article_list]) == 1:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Fake Data Test Accuracy:  {round((correct / total) * 100, 2)}%')\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if dtree.predict([article_list]) == 0:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Real Data Test Accuracy:  {round((correct / total) * 100, 2)}%')"
      ],
      "metadata": {
        "id": "ppaW8I7sSTRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fb6bc5-b0eb-410e-dbb9-1f3506ae1daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake Data Test Accuracy:  93.91%\n",
            "Real Data Test Accuracy:  98.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "df_train = pd.read_csv('news_data.csv')\n",
        "\n",
        "list_of_features = df_train.keys()[:-1]\n",
        "X = df_train[list_of_features].values\n",
        "y = df_train['target_label']\n",
        "\n",
        "#Train KNN\n",
        "KNN = KNeighborsClassifier(n_neighbors = 3)\n",
        "KNN = KNN.fit(X,y)\n"
      ],
      "metadata": {
        "id": "d--n1h5IP7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fake = pd.read_csv(\"Fake.csv\")\n",
        "df_real = pd.read_csv(\"True.csv\")\n",
        "df_fake = df_fake['text']\n",
        "df_real = df_real['text']\n",
        "\n",
        "\n",
        "df_fake = df_fake[1000:10000]\n",
        "df_real = df_real[1000:10000]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_fake:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:                        # try/except is faster than if/else\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue #word not in dictionary, go to next word (just an error catch)\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if KNN.predict([article_list]) == 1:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Fake Data Test Accuracy:  {round((correct / total) * 100, 2)}%')\n",
        "\n",
        "#Repeat process for real articles (label of 0 for true)\n",
        "correct = 0\n",
        "total = 0\n",
        "for text in df_real:\n",
        "  text = clean_article(text)\n",
        "  for word in text:\n",
        "    try:\n",
        "      article_dict[word] += 1\n",
        "    except:\n",
        "      continue\n",
        "  article_list = list(article_dict.values())\n",
        "  article_dict = dict.fromkeys(article_dict, 0)\n",
        "\n",
        "  if KNN.predict([article_list]) == 0:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "print(f'Real Data Test Accuracy:  {round((correct / total) * 100, 2)}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrXBK8eCRpDm",
        "outputId": "114735ca-d766-4ffe-bc18-f037d4d1f0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake Data Test Accuracy:  16.62%\n",
            "Real Data Test Accuracy:  95.63%\n"
          ]
        }
      ]
    }
  ]
}